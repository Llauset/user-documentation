[{"title":"Request a MesoNET account","type":0,"sectionRef":"#","url":"/documentation/user-documentation/connectToMesonet","content":"Request a MesoNET account You need to create a MesoNET account to access all MesoNET services. Account manager is available at https://iam.mesonet.fr Use first the EduGAIN button, select your institution and your institutional identifiers to connectIf you do not have an institution present in EduGAIN, you can create a local account at https://iam.mesonet.fr/start-registration In both cases, you need to choose a username (you future login) and fill your motivation to join. You will next receive a validation mail (check your spam directory) : you need to clic on the validation link. Manual validation by a MesoNET administrator will then be required, which may take a few days. üí•NOTE : During this process, you may be confronted with an error message. In this case, please stop and restart your browser. tip If you have any question, please use the support tool and submit a ticket. Define your username To access mesonet services you are going to need a username. To define a username, you can go to your [profile] with the link or by clicking on the top right icon -&gt; Profile. The username must be entered under the Login field. Access to mesonet services MesoNET website, including description of the ressources : https://www.mesonet.fr GramC portal scientific attribution of the ressources : https://acces.mesonet.fr Temporary portal to access to ressources : https://www.mesonet.fr/portal Documentation portal : TBR Support portal : TBR","keywords":"","version":"Next"},{"title":"Nvidia NGC catalog","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/Apptainer/Building_NGC_Containers","content":"","keywords":"","version":"Next"},{"title":"Pull NGC containers with Apptainer‚Äã","type":1,"pageTitle":"Nvidia NGC catalog","url":"/documentation/user-documentation/juliet/Apptainer/Building_NGC_Containers#pull-ngc-containers-with-apptainer","content":"Pulling containers from NGC requires authentication. Trying to pull an image without an authentification token will result in the following error: $&gt; apptainer pull nvidia_hpc_benchmarks.sif docker://nvcr.io/nvidia/hpc-benchmarks:23.5 FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://nvcr.io/nvidia/hpc-benchmarks:23.5: reading manifest 23.5 in nvcr.io/nvidia/hpc-benchmarks: unauthorized: authentication required  To set up an authentification token follow the steps below: Create an account at https://ngc.nvidia.com or Sign In if you already have an accountOnce logged in, generate an API key at https://ngc.nvidia.com/setupExport the Apptainer environment variables. Execute the commands below after replacing &lt;API_key&gt; with your generated API key. You can add these commands to your .bashrc. export APPTAINER_DOCKER_USERNAME='$oauthtoken' export APPTAINER_DOCKER_PASSWORD=&lt;API_key&gt;  Now you are able to pull any NGC container. For example, you can build the NVIDIA HPC-Benchmarks which includes three benchmarks (HPL-NVIDIA, HPL-AI-NVIDIA and HPCG-NVIDIA) along with some GPU-optimized communication libraries: apptainer pull nvidia_hpc_benchmarks.sif docker://nvcr.io/nvidia/hpc-benchmarks:23.5  ","version":"Next","tagName":"h2"},{"title":"Documentation Utilisateur pour Slurm","type":0,"sectionRef":"#","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm","content":"","keywords":"","version":"Next"},{"title":"1. Introduction √† Slurm‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#1-introduction-√†-slurm","content":"Slurm (Simple Linux Utility for Resource Management) est un puissant syst√®me de gestion de ressources con√ßu pour planifier, surveiller et ex√©cuter des travaux sur des grappes de calcul. Voici quelques concepts de base : Travail (Job) : Une unit√© de travail soumise √† Slurm pour ex√©cution.Partition : Un groupe de n≈ìuds de calcul avec des propri√©t√©s similaires.N≈ìud : Une machine individuelle au sein de la grappe.T√¢che (Task) : Une unit√© d'ex√©cution d'un travail, qui peut √™tre un processus ou un thread.Sbatch : Utilitaire pour soumettre des travaux non interactifs.Srun : Utilitaire pour ex√©cuter des t√¢ches interactives ou non interactives. ","version":"Next","tagName":"h2"},{"title":"2. Soumission de travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#2-soumission-de-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 1 : Soumettre un script simple‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-1--soumettre-un-script-simple","content":"sbatch mon_script.sh  Cette commande soumet le script mon_script.sh pour ex√©cution. Assurez-vous que mon_script.sh contient les directives Slurm appropri√©es en haut du script pour sp√©cifier les ressources n√©cessaires. ","version":"Next","tagName":"h3"},{"title":"Exemple 2 : Soumettre avec allocation de ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-2--soumettre-avec-allocation-de-ressources","content":"sbatch --partition=mesonet --nodes=2 --ntasks-per-node=4 mon_script.sh  Cette commande alloue le travail √† la partition &quot;mesonet&quot; sur 2 n≈ìuds, avec 4 t√¢ches par n≈ìud. Assurez-vous d'ajuster les valeurs selon vos besoins. ","version":"Next","tagName":"h3"},{"title":"3. V√©rification de l'√©tat des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#3-v√©rification-de-l√©tat-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 3 : V√©rifier l'√©tat de vos travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-3--v√©rifier-l√©tat-de-vos-travaux","content":"squeue -u votre_nom_utilisateur  Cette commande affiche la liste des travaux en cours pour l'utilisateur sp√©cifi√©. Vous pouvez voir des informations telles que l'ID du travail, l'√©tat, le n≈ìud, le temps, etc. ","version":"Next","tagName":"h3"},{"title":"Exemple 4 : V√©rifier les travaux de tous les utilisateurs‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-4--v√©rifier-les-travaux-de-tous-les-utilisateurs","content":"squeue  Cette commande affiche la liste de tous les travaux en cours. Utilisez des options suppl√©mentaires pour filtrer les r√©sultats, par exemple, squeue --partition=mesonet pour afficher les travaux dans la partition &quot;mesonet&quot;. ","version":"Next","tagName":"h3"},{"title":"4. Gestion des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#4-gestion-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 5 : Annuler un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-5--annuler-un-travail","content":"scancel ID_du_travail  Cette commande annule le travail avec l'ID sp√©cifi√©. Vous pouvez obtenir l'ID du travail √† partir de la commande squeue. ","version":"Next","tagName":"h3"},{"title":"Exemple 6 : Modifier la priorit√© d'un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-6--modifier-la-priorit√©-dun-travail","content":"scontrol update JobID ID_du_travail Priority=50  Cette commande modifie la priorit√© du travail avec l'ID sp√©cifi√©. La priorit√© affecte l'ordre d'ex√©cution des travaux. ","version":"Next","tagName":"h3"},{"title":"5. Gestion des ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#5-gestion-des-ressources","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 7 : Sp√©cifier les ressources avec sbatch‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-7--sp√©cifier-les-ressources-avec-sbatch","content":"sbatch --partition=compute --nodes=1 --cpus-per-task=8 --mem=16G mon_script.sh  Cette commande sp√©cifie les ressources pour le travail, y compris la partition, le nombre de n≈ìuds, le nombre de t√¢ches par n≈ìud et le nombre de CPU par t√¢che. ","version":"Next","tagName":"h3"},{"title":"Exemple 8 : Ex√©cuter des t√¢ches interactives avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-8--ex√©cuter-des-t√¢ches-interactives-avec-srun","content":"srun --pty -c 4 /bin/bash  Cette commande lance un shell interactif avec 4 CPU allou√©s. Utile pour les t√¢ches interactives ou les tests. ","version":"Next","tagName":"h3"},{"title":"6. Param√®tres avanc√©s‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#6-param√®tres-avanc√©s","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 9 : Utiliser une r√©servation de n≈ìuds‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-9--utiliser-une-r√©servation-de-n≈ìuds","content":"srun --reservation=ma_reservation --nodes=2 mon_script.sh  Cette commande ex√©cute le travail sur une r√©servation de 2 n≈ìuds sp√©cifi√©e avec l'option --reservation. ","version":"Next","tagName":"h3"},{"title":"Exemple 10 : Utiliser une partition sp√©cifique avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-10--utiliser-une-partition-sp√©cifique-avec-srun","content":"srun --partition=visu --nodes=1 mon_script.sh  Cette commande ex√©cute le travail sur la partition &quot;visu&quot; avec 1 n≈ìud. ","version":"Next","tagName":"h3"},{"title":"7. Informations sur les noeuds de calcul‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#7-informations-sur-les-noeuds-de-calcul","content":"sinfo  Cette commande affiche des informations d√©taill√©es sur les n≈ìuds disponibles, telles que leur nom, partition, √©tat, nombre de t√¢ches, m√©moire, ressources, charge CPU, temps d'activit√© et utilisation GPU ","version":"Next","tagName":"h2"},{"title":"8. Ressources suppl√©mentaires‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#8-ressources-suppl√©mentaires","content":"https://slurm.schedmd.com/documentation.html ","version":"Next","tagName":"h2"},{"title":"NVIDIA HPC-Benchmarks","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks","content":"","keywords":"","version":"Next"},{"title":"Running the benchmarks‚Äã","type":1,"pageTitle":"NVIDIA HPC-Benchmarks","url":"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks#running-the-benchmarks","content":"Before proceeding to this section please make sure that you configured the NGC &lt;API_key&gt; for Apptainer see Pull NGC containers with Apptainer for details. caution Nvidia HPL implementation requires gdrcopy to be installed on the compute nodes. You can check if gdrcopy is loaded via the lsmod | grep gdrdrv command. The last one needs to be executed on the compute nodes (there is no reason to have a GPU driver on login nodes). Feel free to perform this operation via slurm if needed: srun -p mesonet -n 1 -w juliet3 lsmod | grep gdrdrv ","version":"Next","tagName":"h2"},{"title":"Pull the NCG container‚Äã","type":1,"pageTitle":"NVIDIA HPC-Benchmarks","url":"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks#pull-the-ncg-container","content":"apptainer pull nvidia_hpc_benchmarks.sif docker://nvcr.io/nvidia/hpc-benchmarks:23.5  ","version":"Next","tagName":"h3"},{"title":"run NVIDIA HPL‚Äã","type":1,"pageTitle":"NVIDIA HPC-Benchmarks","url":"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks#run-nvidia-hpl","content":" srun -p mesonet -N 1 -n 8 -c 14 -G 8 --cpu-bind=none --mpi=pmi2 apptainer exec --nv nvidia_hpc_benchmarks.sif /workspace/hpl.sh --dat /workspace/hpl-linux-x86_64/sample-dat/HPL-dgx-1N.dat --ucx-tls ^cma  info Inside the container, the /workspace/hpl-linux-x86_64/sample-dat directory includes several .dat input files (e.g. inputs suitable for 2,4 and 16 DGX nodes). Feel free to use the most appropriate ones for your benchmarking, or to build your own inputs. ","version":"Next","tagName":"h3"},{"title":"run NVIDIA HPL-AI‚Äã","type":1,"pageTitle":"NVIDIA HPC-Benchmarks","url":"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks#run-nvidia-hpl-ai","content":" srun -p mesonet -N 1 -n 8 -c 14 -G 8 --cpu-bind=none --mpi=pmi2 apptainer exec --nv nvidia_hpc_benchmarks.sif /workspace/hpl.sh --xhpl-ai --gpu-affinity 2:3:0:1:6:7:4:5 --mem-affinity 2:3:0:1:6:7:4:5 --dat /workspace/hpl-ai-linux-x86_64/sample-dat/HPL-dgx-1N.dat --ucx-tls ^cma  info The processor and memory affinities can drastically impact the performance of the HPL-AI-NVIDIA benchmark. Hence, you might need to hand-tune the CPU, GPU, and memory affinities via the --cpu-affinity, --mem-affinity, and --gpu-affinity. For more details see the NGC catalog ","version":"Next","tagName":"h3"},{"title":"run NVIDIA HPCG‚Äã","type":1,"pageTitle":"NVIDIA HPC-Benchmarks","url":"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks#run-nvidia-hpcg","content":"srun -p mesonet -N 1 -n 8 -c 14 -G 8 --cpu-bind=none --mpi=pmi2 apptainer exec --nv nvidia_hpc_benchmarks.sif /workspace/hpcg.sh --nx 256 --ny 256 --nz 256 --rt 2 --ucx-tls ^cma  :::Caution There is a known issue when running HPC-X with Apptainer. It fails because of the cma shared memory fabric support. Hence, for all our runs we disable the cma via the --ucx-tls command line argument. You can also disable it via the UCX_TLS environment variable (i.e. export UCX_TLS=^cma). More details about this issue can be found in the Workaround for Communication Issue with MPI Apps &amp; Apptainer report. ::: ","version":"Next","tagName":"h3"},{"title":"Apptainer and MPI implementations","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/MPI/MPI_and_Apptainer","content":"","keywords":"","version":"Next"},{"title":"Hybrid Model‚Äã","type":1,"pageTitle":"Apptainer and MPI implementations","url":"/documentation/user-documentation/juliet/MPI/MPI_and_Apptainer#hybrid-model","content":"The Apptainer documentation provides several useful examples for the Hybrid model, which can be directly used on the MesoNET supercomputers (i.e. without any additional configurations). Hence tip On Slurm supercomputers, it is possible to avoid having two MPI installation instances. Indeed, by using srun it is possible to launch containerized MPI applications without having any host-side MPI library. In this case, you totally rely on the MPI library that is inside the container. See examples in NVIDIA HPC-Benchmarks. ","version":"Next","tagName":"h2"},{"title":"Bind Model‚Äã","type":1,"pageTitle":"Apptainer and MPI implementations","url":"/documentation/user-documentation/juliet/MPI/MPI_and_Apptainer#bind-model","content":"By default, Apptainer automatically mounts several bind points: $HOME, /sys, /proc, /tmp, etc. Meaning that you will have access to these points inside the container. These bindings depend on the system configuration and can be different from one machine to another. For more details please visit the Apptainer Bind Paths and Mounts Documentation. Hence, in order to use the host-side MPI library, we need to identify its location, and mount/bind it into the container. Below we provide an example of an Aptainer (Definition File)[https://apptainer.org/docs/user/main/definition_files.html] where we use the host-side MPI implementation to install the well-known OSU Micro-Benchmarks inside the container. Then, we run MPI applications inside the container by specifying the installation path to the host-side MPI library via the Apptainer --bind option. caution This example is based on an NGC container. Before proceeding, please make sure that you configured the NGC Catalog &lt;API_key&gt;. On juliet supercomputer, there is a CUDA-Aware MPI library installed in /apps/manual_install/openmpi/4.1.5. This location does not correspond to a path that is mounted by default. Hence, we need to manually specify this location when building the container. Also, since we have a CUDA-Aware communication library, we need CUDA to be installed inside the container. For this, we create a definition file called apptainer_mpi_cuda.sif based on an existing container called nvcr.io/nvidia/cuda:12.2.0-devel-rockylinux9 from the NGC Catalog, which already has CUDA installed. Below is the content of the definition file.  Bootstrap: docker From: nvcr.io/nvidia/cuda:12.2.0-devel-rockylinux9 %setup mkdir -p $APPTAINER_ROOTFS/openmpi/4.1.5 mkdir -p $APPTAINER_ROOTFS/bin_host mkdir -p $APPTAINER_ROOTFS/lib64_host %environment export PATH=&quot;$PATH:/usr/bin_host:/apps/manual_install/openmpi/4.1.5/bin&quot; export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/lib64_host:/apps/manual_install/openmpi/4.1.5/lib&quot; %post dnf install -y wget gcc g++ # relocate MPI installation directory (bind mode) export OPAL_PREFIX=&quot;/openmpi/4.1.5&quot; export PATH=&quot;$PATH:/bin_host:/openmpi/4.1.5/bin&quot; export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/lib64_host:/openmpi/4.1.5/lib&quot; # install OSU MPI benchmarks mkdir /root/network_benchmarking &amp;&amp; cd $_ wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.2.tar.gz tar -xf osu-micro-benchmarks-7.2.tar.gz &amp;&amp; rm osu-micro-benchmarks-7.2.tar.gz cd osu-micro-benchmarks-7.2 ./configure CC=/openmpi/4.1.5/bin/mpicc CXX=/openmpi/4.1.5/bin/mpicxx --enable-cuda --with-cuda-include=/usr/local/cuda-12.2/include --with-cuda-libpath=/usr/local/cuda-12.2/lib64 make  Build the container on juliet: $&gt; export JULIET_MPI_BINDING=/apps/manual_install/openmpi/4.1.5:/openmpi/4.1.5,/usr/bin:/bin_host,/usr/lib64:/lib64_host $&gt; apptainer build --nv --bind $JULIET_MPI_BINDING apptainer_mpi_cuda.sif apptainer_mpi_cuda.def  Run the container on juliet: Run the container on juliet: export JULIET_MPI_BINDING=/apps/manual_install/openmpi/4.1.5:/openmpi/4.1.5,/usr/bin:/bin_host,/usr/lib64:/lib64_host $&gt; apptainer shell --nv --bind $JULIET_MPI_BINDING apptainer_mpi_cuda.sif  info The juliet MPI instance depends on the hwloc library, which is located in /usr directory. Hence we also need to bind the /usr/bin and /usr/lib directories. ","version":"Next","tagName":"h2"},{"title":"Documentation Utilisateur pour Slurm","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm","content":"","keywords":"","version":"Next"},{"title":"1. Introduction √† Slurm‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#1-introduction-√†-slurm","content":"Slurm (Simple Linux Utility for Resource Management) est un puissant syst√®me de gestion de ressources con√ßu pour planifier, surveiller et ex√©cuter des travaux sur des grappes de calcul. Voici quelques concepts de base : Travail (Job) : Une unit√© de travail soumise √† Slurm pour ex√©cution.Partition : Un groupe de n≈ìuds de calcul avec des propri√©t√©s similaires.N≈ìud : Une machine individuelle au sein de la grappe.T√¢che (Task) : Une unit√© d'ex√©cution d'un travail, qui peut √™tre un processus ou un thread.Sbatch : Utilitaire pour soumettre des travaux non interactifs.Srun : Utilitaire pour ex√©cuter des t√¢ches interactives ou non interactives. ","version":"Next","tagName":"h2"},{"title":"2. Soumission de travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#2-soumission-de-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 1 : Soumettre un script simple‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-1--soumettre-un-script-simple","content":"sbatch mon_script.sh  Cette commande soumet le script mon_script.sh pour ex√©cution. Assurez-vous que mon_script.sh contient les directives Slurm appropri√©es en haut du script pour sp√©cifier les ressources n√©cessaires. ","version":"Next","tagName":"h3"},{"title":"Exemple 2 : Soumettre avec allocation de ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-2--soumettre-avec-allocation-de-ressources","content":"sbatch --partition=mesonet --nodes=2 --ntasks-per-node=4 mon_script.sh  Cette commande alloue le travail √† la partition &quot;mesonet&quot; sur 2 n≈ìuds, avec 4 t√¢ches par n≈ìud. Assurez-vous d'ajuster les valeurs selon vos besoins. ","version":"Next","tagName":"h3"},{"title":"3. V√©rification de l'√©tat des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#3-v√©rification-de-l√©tat-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 3 : V√©rifier l'√©tat de vos travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-3--v√©rifier-l√©tat-de-vos-travaux","content":"squeue -u votre_nom_utilisateur  Cette commande affiche la liste des travaux en cours pour l'utilisateur sp√©cifi√©. Vous pouvez voir des informations telles que l'ID du travail, l'√©tat, le n≈ìud, le temps, etc. ","version":"Next","tagName":"h3"},{"title":"Exemple 4 : V√©rifier les travaux de tous les utilisateurs‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-4--v√©rifier-les-travaux-de-tous-les-utilisateurs","content":"squeue  Cette commande affiche la liste de tous les travaux en cours. Utilisez des options suppl√©mentaires pour filtrer les r√©sultats, par exemple, squeue --partition=mesonet pour afficher les travaux dans la partition &quot;mesonet&quot;. ","version":"Next","tagName":"h3"},{"title":"4. Gestion des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#4-gestion-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 5 : Annuler un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-5--annuler-un-travail","content":"scancel ID_du_travail  Cette commande annule le travail avec l'ID sp√©cifi√©. Vous pouvez obtenir l'ID du travail √† partir de la commande squeue. ","version":"Next","tagName":"h3"},{"title":"Exemple 6 : Modifier la priorit√© d'un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-6--modifier-la-priorit√©-dun-travail","content":"scontrol update JobID ID_du_travail Priority=50  Cette commande modifie la priorit√© du travail avec l'ID sp√©cifi√©. La priorit√© affecte l'ordre d'ex√©cution des travaux. ","version":"Next","tagName":"h3"},{"title":"5. Gestion des ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#5-gestion-des-ressources","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 7 : Sp√©cifier les ressources avec sbatch‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-7--sp√©cifier-les-ressources-avec-sbatch","content":"sbatch --partition=compute --nodes=1 --cpus-per-task=8 --mem=16G mon_script.sh  Cette commande sp√©cifie les ressources pour le travail, y compris la partition, le nombre de n≈ìuds, le nombre de t√¢ches par n≈ìud et le nombre de CPU par t√¢che. ","version":"Next","tagName":"h3"},{"title":"Exemple 8 : Ex√©cuter des t√¢ches interactives avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-8--ex√©cuter-des-t√¢ches-interactives-avec-srun","content":"srun --pty -c 4 /bin/bash  Cette commande lance un shell interactif avec 4 CPU allou√©s. Utile pour les t√¢ches interactives ou les tests. ","version":"Next","tagName":"h3"},{"title":"6. Param√®tres avanc√©s‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#6-param√®tres-avanc√©s","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 9 : Utiliser une r√©servation de n≈ìuds‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-9--utiliser-une-r√©servation-de-n≈ìuds","content":"srun --reservation=ma_reservation --nodes=2 mon_script.sh  Cette commande ex√©cute le travail sur une r√©servation de 2 n≈ìuds sp√©cifi√©e avec l'option --reservation. ","version":"Next","tagName":"h3"},{"title":"Exemple 10 : Utiliser une partition sp√©cifique avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#exemple-10--utiliser-une-partition-sp√©cifique-avec-srun","content":"srun --partition=visu --nodes=1 mon_script.sh  Cette commande ex√©cute le travail sur la partition &quot;visu&quot; avec 1 n≈ìud. ","version":"Next","tagName":"h3"},{"title":"7. Informations sur les noeuds de calcul‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#7-informations-sur-les-noeuds-de-calcul","content":"sinfo  Cette commande affiche des informations d√©taill√©es sur les n≈ìuds disponibles, telles que leur nom, partition, √©tat, nombre de t√¢ches, m√©moire, ressources, charge CPU, temps d'activit√© et utilisation GPU ","version":"Next","tagName":"h2"},{"title":"8. Ressources suppl√©mentaires‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/juliet/Doc_Utilisateur_Slurm#8-ressources-suppl√©mentaires","content":"https://slurm.schedmd.com/documentation.html ","version":"Next","tagName":"h2"},{"title":"Request access to a server","type":0,"sectionRef":"#","url":"/documentation/user-documentation/requestAccess","content":"Request access to a server Prerequisites 1) You need a validated MesoNET account. You can find the steps to obtain one here. 2) You need to have entered a Login in your MesoNET profile. 3) You need a valid SSH key. You can generate a key in a command line interface with the command : ssh-keygen Access request Juliet AI server The ticket center is currently under construction. In the meantime you can request access to the Juliet AI pod through the Romeo ticket center To request access : - Select &quot;MESONET Account Creation&quot; under &quot;Help Topic&quot; - Provide an e-mail address to contact the owner of the account - Provide the username in the details of the ticket. Add a ssh key This step can be performed before or after the previous one. Go to your key list with the link or by clicking on the top right icon -&gt; My keys. At the bottom of the list, click on New Key. Enter a Label for your key to identify it info Labels are not unique and are only used for information. Enter the type and value of your public key on the Key field. note Examples of the expected format : ssh-ed25519 ThATiSthEB0DY0FAKeY ssh-rsa ThisTypeOfKeyIsMuchLongerThanThatButYouGetTheIdea Check the Valid Key button to validate your key. You can have as many valid keys as you want. caution ‚ö† In the current state, the portal has no failsafe to detect invalid or duplicate SSH keys. ‚ö† ‚ö† Please make sure your key is valid before adding it. ‚ö† Depending on what you copied it from, some newline character might have been introduced in the text and must be removed. Activate SSH key Once you have been granted access to a server, it should appear on the Dashboard By clicking on a computing architecture you can select which keys are activated or deactivate for this architecture. To activate a SSH key for juliet, click on the gray button next to its label. The button will turn green. The key will be usable on juliet once the list has been refreshed. To deactivate a SSH key, click the same button. It will turn from green to gray. info Juliet AI server On Juliet, the list of activated keys is refreshed every 3 hours. Revoke SSH Key To revoke a SSH Key, go to your key list with the link or by clicking on the top right icon -&gt; My keys. Click on the key label. Finally under Validity, check Unvalid key. caution This will revoke the key for every MesoNET server. ‚ö† In the current state, the portal has no failsafe to detect duplicate SSH keys. ‚ö† If there are duplicate SSH keys revoking one will also revoke the others. tip SSH keys that are entered cannot be modified or deleted. If you want to edit a key, you will need to revoke the first one and create a new one.","keywords":"","version":"Next"},{"title":"Building a CUDA-aware Open MPI library on an Infiniband cluster","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide","content":"","keywords":"","version":"Next"},{"title":"Setting up and building UCX with GPU support‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-ucx-with-gpu-support","content":"","version":"Next","tagName":"h2"},{"title":"GPU communication performance‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#gpu-communication-performance","content":"Before building UCX, we might need to set up the GPUDirect RDMA technology, in order to improve the GPU communication performance. Prior to CUDA 11.4, GPUDirect RDMA technology was handled by thenv_peer_memory kernel developed by Mellanox. Starting with CUDA 11.4 there is a new kernel module called nvidia-peermem implemented by Nvidia. To note that nv_peer_memory became deprecated and should be replaced by nvidia-peermem. Please visit the Nvidia website for more details. Additionally, to optimize the intra-node GPU communication latency, UCX should be build with the gdrcopy support. The last one is a library based on the GPUDirect RDMA features. A data transfer performed with gdrcopy is driven by the CPU, and is meant to reduce the communication latency. This library is composed of a kernel module called gdrdrv and a API called gdrapi. Hence, before building UCX please make sure that both nvidia-peermem and gdrdrv kernel modules are installed and loaded. $ lsmod | grep gdrdrv $ lsmod | grep nvidia_peermem  If the kernel modules are not loaded please refer to the following resources for installing gdrcopy and/or for loading nvidia-peermem. ","version":"Next","tagName":"h3"},{"title":"Building UCX‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-ucx","content":"Building UCX is typically a combination of running &quot;configure&quot; and &quot;make&quot;. For GPU support we need to specify the cuda and gdrcopy install directories via the --with-cuda and --with-gdrcopy options of &quot;configure&quot;. Below are the steps required to build and install UCX on juliet supercomputer. The latest release UCX tarball can be downloaded from the UCX repository. $ ./configure --prefix=&lt;prefix_path&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-gdrcopy=/apps/manual_install/gdrcopy $ make -j8 install  Once the installation is completed, the information about the current UCX installation instance can be retrieved via the ucx_info command. For example, it is possible to check the UCX GPU support via the following command: $ ucx_info -d | grep cuda  To obtain more information it is also possible to change the UCX log level: $ env UCX_LOG_LEVEL=debug ucx_info -d | grep -i cuda  ","version":"Next","tagName":"h3"},{"title":"Setting up and building OpenMPI‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-openmpi","content":"Below are the commands for building a CUDA-Aware OpenMPI library with Slurm support on juliet supercomputer. We need to specify the path to the UCX installation directory via the --with-ucx option and the path to cuda via --with-cuda. Additionally, we need to set the --with-pmi option for supporting slurm (i.e. running MPI application with srun). We also disable the btl openib via --without-verbs option. $ ./configure --prefix=&lt;prefix_path&gt; --with-ucx=&lt;path_to_ucx_install&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-pmi --without-verbs make -j8 install # Check that OpenMPI has been built with CUDA-aware support: $ ompi_info --parsable --all | grep mpi_built_with_cuda_support:value  note Recent OpenMPI versions contain a BTL component called uct, which might cause data corruption when building MPI with UCX. If needed, you can disable uct via the --enable-mca-no-build=btl-uct configuration option. More information about building MPI with UCX can be found here. ","version":"Next","tagName":"h2"},{"title":"OpenMPI performance tests‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#openmpi-performance-tests","content":"For testing the performance of a CUDA-Aware OpenMPI installation instance, one can use the well-known OSU benchmarks. ","version":"Next","tagName":"h2"},{"title":"Building OSU Micro-Benchmarks‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-osu-micro-benchmarks","content":"$ wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.2.tar.gz $ tar xfp osu-micro-benchmarks-7.2.tar.gz $ cd osu-micro-benchmarks-7.2 $ ./configure CC=mpicc CXX=mpicxx --prefix=&lt;prefix&gt; --enable-cuda $ make install  ","version":"Next","tagName":"h3"},{"title":"Running OSU Micro-Benchmarks‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#running-osu-micro-benchmarks","content":"By default, UCX tries to use all available devices on the machine and selects the best ones based on performance characteristics. One can also use manual tunning in order to force the use of certain devices or technologies. For example, we can enable or disable the use of GPUDirect RDMA optimization (available through the nvidia-peermem kernel) and enable or disable the use of gdrcopy (i.e. use of the gdrdrv kernel). Below are a few examples. Performance graphGPUDirect RDMA and gdrcopyGPUDirect RDMA onlygdrcopy onlyno GDR optimization Manual tunning parameters‚Äã For all the runs above we need to specify the location of the gdrcopy library via -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/manual_install/gdrcopy/lib. Without this option, the MPI library could not use gdrcopy, resulting in performance degradation. We also select the same Infiniband MCA for all our runs via x UCX_NET_DEVICES=mlx5_0:1 . This was done for reproducibility issues. To learn more about possible UCX options and manual tunning please visit the OpenUCX website. ","version":"Next","tagName":"h3"}]