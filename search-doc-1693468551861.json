[{"title":"Request a mesonet account","type":0,"sectionRef":"#","url":"/documentation/user-documentation/connectToMesonet","content":"Request a mesonet account You need to create a MesoNET account to access all MesoNET services. Account manager is available at https://iam.mesonet.fr Use first the EduGAIN button, select your institution and your institutional identifiers to connectWithout an institution present in EduGAIN, you can create a local account at https://iam.mesonet.fr/start-registration In both cases, you need to choose a username (you future login) and fill your motivation to join. You will next receive a validation mail : you need to clic on the validation link. Manual validation by a MesoNET administrator will then be required, which may take a few days. üí•NOTE : During this process, you may be confronted with an error message. In this case, please stop and restart your browser. tip If you have any questions, please use the support tool and submit a ticket. Access to mesonet services MesoNET website, including description of the ressources : https://www.mesonet.fr GramC portal scientific attribution of the ressources : https://acces.mesonet.fr Temporary portal to access to ressources : https://www.mesonet.fr/portal Documentation portal : TBR Support portal : TBR","keywords":"","version":"Next"},{"title":"Documentation Utilisateur pour Slurm","type":0,"sectionRef":"#","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm","content":"","keywords":"","version":"Next"},{"title":"1. Introduction √† Slurm‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#1-introduction-√†-slurm","content":"Slurm (Simple Linux Utility for Resource Management) est un puissant syst√®me de gestion de ressources con√ßu pour planifier, surveiller et ex√©cuter des travaux sur des grappes de calcul. Voici quelques concepts de base : Travail (Job) : Une unit√© de travail soumise √† Slurm pour ex√©cution.Partition : Un groupe de n≈ìuds de calcul avec des propri√©t√©s similaires.N≈ìud : Une machine individuelle au sein de la grappe.T√¢che (Task) : Une unit√© d'ex√©cution d'un travail, qui peut √™tre un processus ou un thread.Sbatch : Utilitaire pour soumettre des travaux non interactifs.Srun : Utilitaire pour ex√©cuter des t√¢ches interactives ou non interactives. ","version":"Next","tagName":"h2"},{"title":"2. Soumission de travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#2-soumission-de-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 1 : Soumettre un script simple‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-1--soumettre-un-script-simple","content":"sbatch mon_script.sh  Cette commande soumet le script mon_script.sh pour ex√©cution. Assurez-vous que mon_script.sh contient les directives Slurm appropri√©es en haut du script pour sp√©cifier les ressources n√©cessaires. ","version":"Next","tagName":"h3"},{"title":"Exemple 2 : Soumettre avec allocation de ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-2--soumettre-avec-allocation-de-ressources","content":"sbatch --partition=mesonet --nodes=2 --ntasks-per-node=4 mon_script.sh  Cette commande alloue le travail √† la partition &quot;mesonet&quot; sur 2 n≈ìuds, avec 4 t√¢ches par n≈ìud. Assurez-vous d'ajuster les valeurs selon vos besoins. ","version":"Next","tagName":"h3"},{"title":"3. V√©rification de l'√©tat des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#3-v√©rification-de-l√©tat-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 3 : V√©rifier l'√©tat de vos travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-3--v√©rifier-l√©tat-de-vos-travaux","content":"squeue -u votre_nom_utilisateur  Cette commande affiche la liste des travaux en cours pour l'utilisateur sp√©cifi√©. Vous pouvez voir des informations telles que l'ID du travail, l'√©tat, le n≈ìud, le temps, etc. ","version":"Next","tagName":"h3"},{"title":"Exemple 4 : V√©rifier les travaux de tous les utilisateurs‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-4--v√©rifier-les-travaux-de-tous-les-utilisateurs","content":"squeue  Cette commande affiche la liste de tous les travaux en cours. Utilisez des options suppl√©mentaires pour filtrer les r√©sultats, par exemple, squeue --partition=mesonet pour afficher les travaux dans la partition &quot;mesonet&quot;. ","version":"Next","tagName":"h3"},{"title":"4. Gestion des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#4-gestion-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 5 : Annuler un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-5--annuler-un-travail","content":"scancel ID_du_travail  Cette commande annule le travail avec l'ID sp√©cifi√©. Vous pouvez obtenir l'ID du travail √† partir de la commande squeue. ","version":"Next","tagName":"h3"},{"title":"Exemple 6 : Modifier la priorit√© d'un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-6--modifier-la-priorit√©-dun-travail","content":"scontrol update JobID ID_du_travail Priority=50  Cette commande modifie la priorit√© du travail avec l'ID sp√©cifi√©. La priorit√© affecte l'ordre d'ex√©cution des travaux. ","version":"Next","tagName":"h3"},{"title":"5. Gestion des ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#5-gestion-des-ressources","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 7 : Sp√©cifier les ressources avec sbatch‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-7--sp√©cifier-les-ressources-avec-sbatch","content":"sbatch --partition=compute --nodes=1 --cpus-per-task=8 --mem=16G mon_script.sh  Cette commande sp√©cifie les ressources pour le travail, y compris la partition, le nombre de n≈ìuds, le nombre de t√¢ches par n≈ìud et le nombre de CPU par t√¢che. ","version":"Next","tagName":"h3"},{"title":"Exemple 8 : Ex√©cuter des t√¢ches interactives avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-8--ex√©cuter-des-t√¢ches-interactives-avec-srun","content":"srun --pty -c 4 /bin/bash  Cette commande lance un shell interactif avec 4 CPU allou√©s. Utile pour les t√¢ches interactives ou les tests. ","version":"Next","tagName":"h3"},{"title":"6. Param√®tres avanc√©s‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#6-param√®tres-avanc√©s","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 9 : Utiliser une r√©servation de n≈ìuds‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-9--utiliser-une-r√©servation-de-n≈ìuds","content":"srun --reservation=ma_reservation --nodes=2 mon_script.sh  Cette commande ex√©cute le travail sur une r√©servation de 2 n≈ìuds sp√©cifi√©e avec l'option --reservation. ","version":"Next","tagName":"h3"},{"title":"Exemple 10 : Utiliser une partition sp√©cifique avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-10--utiliser-une-partition-sp√©cifique-avec-srun","content":"srun --partition=visu --nodes=1 mon_script.sh  Cette commande ex√©cute le travail sur la partition &quot;visu&quot; avec 1 n≈ìud. ","version":"Next","tagName":"h3"},{"title":"7. Informations sur les noeuds de calcul‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#7-informations-sur-les-noeuds-de-calcul","content":"sinfo  Cette commande affiche des informations d√©taill√©es sur les n≈ìuds disponibles, telles que leur nom, partition, √©tat, nombre de t√¢ches, m√©moire, ressources, charge CPU, temps d'activit√© et utilisation GPU ","version":"Next","tagName":"h2"},{"title":"8. Ressources suppl√©mentaires‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#8-ressources-suppl√©mentaires","content":"https://slurm.schedmd.com/documentation.html ","version":"Next","tagName":"h2"},{"title":"Building a CUDA-aware Open MPI library on an Infiniband cluster","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide","content":"","keywords":"","version":"Next"},{"title":"Setting up and building UCX with GPU support‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-ucx-with-gpu-support","content":"","version":"Next","tagName":"h2"},{"title":"GPU communication performance‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#gpu-communication-performance","content":"To obtain high communication performance on a Infiniband cluster, we first need to enable the GPUDirect RDMA technology, before building UCX. Prior to CUDA 11.4, GPUDirect RDMA was enabled by installing thenv_peer_memory kernel developped by Mellanox. Starting with CUDA 11.4 there is a new kernel module called nvidia-peermem implemented by Nvidia. To note th\\t nv_peer_memory became deprecated and should be replaced by nvidia-peermem. Please visit the Nvidia website for more details. Additionally, to optimize the intra-node GPU communication latency, UCX should be build with the gdrcopy support. The last one is a library based on the GPUDirect RDMA features. A data transfer performed with gdrcopy is driven by the CPU, and is meant to reduce the communication latency. This library is composed of a kernel module called gdrdrv and a API called gdrapi. Hence, before building UCX please make sure that both nvidia-peermem and gdrdrv kernel modules are installed and loaded. $ lsmod | grep gdrdrv $ lsmod | grep nvidia_peermem  If the kernel modules are not loaded please refer to the following ressources for installing gdrcopy and/or for loading nvidia-peermem. ","version":"Next","tagName":"h3"},{"title":"Building UCX‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-ucx","content":"Building UCX is typically a combination of running &quot;configure&quot; and &quot;make&quot;. For GPU support we need to specify the cuda and gdrcopy install directories via the --with-cuda and --with-gdrcopy options of &quot;configure&quot;. Below are the steps required to build and install UCX on juliet supercomputer. The latest release UCX tarball can be downloaded from the UCX repository. $ ./configure --prefix=&lt;prefix_path&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-gdrcopy=/apps/manual_install/gdrcopy $ make -j8 install  Once the installation is completed, the information about the current UCX installation instance can be retrieved via the ucx_info command. For example, it is possible to check the UCX GPU support via the following command: $ ucx_info -d | grep cuda  To obtain more information it is also possible to change the UCX log level: $ env UCX_LOG_LEVEL=debug ucx_info -d | grep -i cuda  ","version":"Next","tagName":"h3"},{"title":"Setting up and building OpenMPI‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-openmpi","content":"Below are the commands for building a CUDA-Aware OpenMPI library with Slurm support on juliet supercomputer. We need to specify the path to the UCX installation directory via the --with-ucx option and the path to cuda via --with-cuda. Additionaly, we need to set the --with-pmi option for supporting slurm (i.e. running MPI application with srun). We also disable the btl openib via --without-verbs option. $ ./configure --prefix=&lt;prefix_path&gt; --with-ucx=&lt;path_to_ucx_install&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-pmi --without-verbs make -j8 install # Check that OpenMPI has been built with CUDA-aware support: $ ompi_info --parsable --all | grep mpi_built_with_cuda_support:value  note Recent OpenMPI versions contain a BTL component called uct, which might cause data corruption when building MPI with UCX. If needed, you can disable uct via the --enable-mca-no-build=btl-uct configuration option. More information about building MPI with UCX can be found here. ","version":"Next","tagName":"h2"},{"title":"OpenMPI performance tests‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#openmpi-performance-tests","content":"For testing the performance of a CUDA-Aware OpenMPI installation instance, one can use the well-known OSU benchmarks. ","version":"Next","tagName":"h2"},{"title":"Building OSU Micro-Benchmarks‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-osu-micro-benchmarks","content":"$ wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.2.tar.gz $ tar xfp osu-micro-benchmarks-7.2.tar.gz $ cd osu-micro-benchmarks-7.2 $ ./configure CC=mpicc CXX=mpicxx --prefix=&lt;prefix&gt; --enable-cuda $ make install  ","version":"Next","tagName":"h3"},{"title":"Running OSU Micro-Benchmarks‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#running-osu-micro-benchmarks","content":"By default, UCX tries to use all available devices on the machine, and selects best ones based on performance characteristics. One can also use manual tunning in order to force the use of certain devices or technologies. For example, we can enable or disable the use of GPUDirect RDMA optimization (available through the nvidia-peermem kernel) and enable or disable the use of gdrcopy (i.e. use of the gdrdrv kernel). Below are a few examples. GPUDirect RDMA and gdrcopyGPUDirect RDMA onlygdrcopy onlyno GDR optimization Both GPUDirect RDMA and gdrcopy enabled (Device to Device) $ mpirun -H juliet3,juliet4 -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/manual_install/gdrcopy/lib -x UCX_IB_GPU_DIRECT_RDMA=1 -x UCX_TLS=all -x UCX_NET_DEVICES=mlx5_0:1 -np 2 c/mpi/pt2pt/standard/osu_latency -d cuda D D # OSU MPI-CUDA Latency Test v7.2 # Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D) # Size Latency (us) # Datatype: MPI_CHAR. 1 4.06 2 4.26 4 4.33 8 4.22 16 4.23 32 4.37 64 4.45 128 4.59 256 4.54 512 4.88 1024 4.89 2048 4.93 4096 5.57 8192 6.20 16384 9.02 32768 12.15 65536 18.42 131072 31.48 262144 56.07 524288 43.67 1048576 74.08 2097152 145.56 4194304 281.57 ```  Manual tunning parameters‚Äã For all the runs above we need to specify the location of the gdrcopy library via -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/manual_install/gdrcopy/lib. Without this option, the MPI library could not use gdrcopy, resulting in performance degradation. We also select the same Infiniband MCA for all our runs via x UCX_NET_DEVICES=mlx5_0:1 . This was done for reproducibility issues. To learn more about possible UCX options and manual tunning please visit the OpenUCX website. ","version":"Next","tagName":"h3"}]