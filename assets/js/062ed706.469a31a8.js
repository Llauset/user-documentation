"use strict";(self.webpackChunkmeso_net=self.webpackChunkmeso_net||[]).push([[704],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>h});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),p=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=p(e.components);return i.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(n),m=a,h=c["".concat(l,".").concat(m)]||c[m]||u[m]||r;return n?i.createElement(h,o(o({ref:t},d),{},{components:n})):i.createElement(h,o({ref:t},d))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:a,o[1]=s;for(var p=2;p<r;p++)o[p]=n[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}m.displayName="MDXCreateElement"},6065:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var i=n(7462),a=(n(7294),n(3905));const r={},o="Apptainer and MPI implementations",s={unversionedId:"juliet/MPI/MPI_and_Apptainer",id:"juliet/MPI/MPI_and_Apptainer",title:"Apptainer and MPI implementations",description:"There are two main ways of launching MPI applications with apptainer, known as Hybrid and Bind models. The Hybrid model involves the use of two MPI libraries: the MPI installed by the administrator on the host-side, and the MPI installed inside the container. Both libraries work in tandem work in tandem to instantiate the job. Hence, the MPI used to compile the application in the container must be compatible with the version of MPI available on the host. In the case of the Bind model, we mount the host MPI into the container, without installing any MPI library inside the container. The two models, along with their advantages and drawbacks are described in the Apptainer User Guide.",source:"@site/docs/juliet/MPI/MPI_and_Apptainer.md",sourceDirName:"juliet/MPI",slug:"/juliet/MPI/MPI_and_Apptainer",permalink:"/documentation/user-documentation/juliet/MPI/MPI_and_Apptainer",draft:!1,editUrl:"https://github.com/MesoNET/user-documentation/tree/main/docs/juliet/MPI/MPI_and_Apptainer.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Building a CUDA-aware Open MPI  library on an Infiniband cluster",permalink:"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide"},next:{title:"NVIDIA HPC-Benchmarks",permalink:"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks"}},l={},p=[{value:"Hybrid Model",id:"hybrid-model",level:2}],d={toc:p},c="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,i.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"apptainer-and-mpi-implementations"},"Apptainer and MPI implementations"),(0,a.kt)("p",null,"There are two main ways of launching MPI applications with apptainer, known as ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"Bind")," models. The ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," model involves the use of two MPI libraries: the MPI installed by the administrator on the host-side, and the MPI installed inside the container. Both libraries work in tandem work in tandem to instantiate the job. Hence, the MPI used to compile the application in the container must be compatible with the version of MPI available on the host. In the case of the ",(0,a.kt)("inlineCode",{parentName:"p"},"Bind")," model, we mount the host MPI into the container, without installing any MPI library inside the container. The two models, along with their advantages and drawbacks are described in the ",(0,a.kt)("a",{parentName:"p",href:"https://apptainer.org/docs/user/1.0/mpi.html"},"Apptainer User Guide"),"."),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"The present document is inteended to offer some particular examples for MesoNET supercomputer and should not be taken as an comprehensive User Guide. We encourage you to read and uderstand the ",(0,a.kt)("a",{parentName:"p",href:"https://apptainer.org/docs/user/1.0/mpi.html"},"Apptainer Documentation")," before proceeding.")),(0,a.kt)("h2",{id:"hybrid-model"},"Hybrid Model"),(0,a.kt)("p",null,"The Apptainer documentation provides several useful examples for the ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," model. They can be directly used on the MesoNET supercomputers, without any additional configurations."),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"On Slurm supercomputers, it is possible to avoid having two MPI installation instances. Indeed, by using ",(0,a.kt)("inlineCode",{parentName:"p"},"srun")," we can launch containerized MPI applications without having any host-side MPI library. Thus we use exlusivly the MPI library that inside the container, launched with Slurm. ")))}u.isMDXComponent=!0}}]);