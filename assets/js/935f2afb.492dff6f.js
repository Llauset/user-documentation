"use strict";(self.webpackChunkmeso_net=self.webpackChunkmeso_net||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Request a MesoNET account","href":"/documentation/user-documentation/connectToMesonet","docId":"connectToMesonet"},{"type":"link","label":"Request access to a server","href":"/documentation/user-documentation/requestAccess","docId":"requestAccess"},{"type":"link","label":"Documentation Utilisateur pour Slurm","href":"/documentation/user-documentation/Doc_Utilisateur_Slurm","docId":"Doc_Utilisateur_Slurm"},{"type":"link","label":"MesoNET user documentation","href":"/documentation/user-documentation/","docId":"README"},{"type":"category","label":"juliet","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Apptainer","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Nvidia NGC catalog","href":"/documentation/user-documentation/juliet/Apptainer/Building_NGC_Containers","docId":"juliet/Apptainer/Building_NGC_Containers"}]},{"type":"category","label":"MPI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Building a CUDA-aware Open MPI  library on an Infiniband cluster","href":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide","docId":"juliet/MPI/MPI_CUDA_aware_Installation_Guide"}]},{"type":"link","label":"adresses_vm","href":"/documentation/user-documentation/juliet/adresses_vm","docId":"juliet/adresses_vm"},{"type":"category","label":"benchmarks","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA HPC-Benchmarks","href":"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks","docId":"juliet/benchmarks/Nvidia_HPC_Benchmarks"}]}]}]},"docs":{"connectToMesonet":{"id":"connectToMesonet","title":"Request a MesoNET account","description":"You need to create a MesoNET account to access all MesoNET services.","sidebar":"tutorialSidebar"},"Doc_Utilisateur_Slurm":{"id":"Doc_Utilisateur_Slurm","title":"Documentation Utilisateur pour Slurm","description":"1. Introduction \xe0 Slurm","sidebar":"tutorialSidebar"},"juliet/adresses_vm":{"id":"juliet/adresses_vm","title":"adresses_vm","description":"| hostname | MAC address | IP address |","sidebar":"tutorialSidebar"},"juliet/Apptainer/Building_NGC_Containers":{"id":"juliet/Apptainer/Building_NGC_Containers","title":"Nvidia NGC catalog","description":"NVIDIA NGC catalog provides a set of containerized environments (e.g. software development kits) you can use in deep learning, machine learning, and high-performance computing projects. Certain containers also include pre-trained models and HPC applications, optimized for running on Nvidia GPUs.","sidebar":"tutorialSidebar"},"juliet/benchmarks/Nvidia_HPC_Benchmarks":{"id":"juliet/benchmarks/Nvidia_HPC_Benchmarks","title":"NVIDIA HPC-Benchmarks","description":"Nvidia HPC-Benchmarks represent a collection of three classical benchmarks (HPL, HPL-AI, and HPCG) optimized for Nvidia accelerated systems. HPL and HPL-AI are both implementations of the well-known Linpack benchmark. The difference is that HPL solves a dense linear system in double precision (64 bits) arithmetic and HPL-AI in mixed precision arithmetic. Opting for mixed precision can drastically decrease the execution time. It can be useful for applications that can achieve desired results at lower floating-point precision formats (e.g. machine learning). HPCG is another well-known benchmark. It performs a fixed number of multigrid preconditioned conjugate gradient iterations using double-precision arithmetics.","sidebar":"tutorialSidebar"},"juliet/MPI/MPI_CUDA_aware_Installation_Guide":{"id":"juliet/MPI/MPI_CUDA_aware_Installation_Guide","title":"Building a CUDA-aware Open MPI  library on an Infiniband cluster","description":"Historically, on Infiniband clusters, Open MPI was built with the openib BTL support, enabled when Open MPI is configured --with-verbs. The openib BTL became deprecated in favor of the Unified Communication X (UCX) PML. Hence, the current documentation shows some guidelines on how to install a CUDA-Aware Open MPI library with UCX. Note that all the examples used in this document were run on juliet supercomputer.","sidebar":"tutorialSidebar"},"README":{"id":"README","title":"MesoNET user documentation","description":"Welcome to MesoNET documentation for users","sidebar":"tutorialSidebar"},"requestAccess":{"id":"requestAccess","title":"Request access to a server","description":"1) You need a validated MesoNET account. You can find the steps to obtain one here//www.mesonet.fr/documentation/user-documentation/connectToMesonet","sidebar":"tutorialSidebar"}}}')}}]);