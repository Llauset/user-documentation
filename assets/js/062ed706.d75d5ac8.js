"use strict";(self.webpackChunkmeso_net=self.webpackChunkmeso_net||[]).push([[704],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>u});var i=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function p(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=i.createContext({}),l=function(e){var n=i.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d=function(e){var n=l(e.components);return i.createElement(s.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},h=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,d=p(e,["components","mdxType","originalType","parentName"]),c=l(t),h=a,u=c["".concat(s,".").concat(h)]||c[h]||m[h]||o;return t?i.createElement(u,r(r({ref:n},d),{},{components:t})):i.createElement(u,r({ref:n},d))}));function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,r=new Array(o);r[0]=h;var p={};for(var s in n)hasOwnProperty.call(n,s)&&(p[s]=n[s]);p.originalType=e,p[c]="string"==typeof e?e:a,r[1]=p;for(var l=2;l<o;l++)r[l]=t[l];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}h.displayName="MDXCreateElement"},6065:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>p,toc:()=>l});var i=t(7462),a=(t(7294),t(3905));const o={},r="Apptainer and MPI implementations",p={unversionedId:"juliet/MPI/MPI_and_Apptainer",id:"juliet/MPI/MPI_and_Apptainer",title:"Apptainer and MPI implementations",description:"There are two main ways of launching MPI applications with apptainer, known as Hybrid and Bind models. The Hybrid model involves the use of two MPI libraries: the MPI installed by the administrator on the host-side, and the MPI installed inside the container. Both libraries work in tandem to instantiate the job. Hence, the MPI used to compile the application in the container must be compatible with the version of MPI available on the host. In the case of the Bind model, we mount the host MPI into the container, without having any MPI library inside the container. The two models, along with their advantages and drawbacks are described in the Apptainer User Guide.",source:"@site/docs/juliet/MPI/MPI_and_Apptainer.md",sourceDirName:"juliet/MPI",slug:"/juliet/MPI/MPI_and_Apptainer",permalink:"/documentation/user-documentation/juliet/MPI/MPI_and_Apptainer",draft:!1,editUrl:"https://github.com/MesoNET/user-documentation/tree/main/docs/juliet/MPI/MPI_and_Apptainer.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Building a CUDA-aware Open MPI  library on an Infiniband cluster",permalink:"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide"},next:{title:"NVIDIA HPC-Benchmarks",permalink:"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks"}},s={},l=[{value:"Hybrid Model",id:"hybrid-model",level:2},{value:"Bind Model",id:"bind-model",level:2}],d={toc:l},c="wrapper";function m(e){let{components:n,...t}=e;return(0,a.kt)(c,(0,i.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"apptainer-and-mpi-implementations"},"Apptainer and MPI implementations"),(0,a.kt)("p",null,"There are two main ways of launching MPI applications with apptainer, known as ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"Bind")," models. The ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," model involves the use of two MPI libraries: the MPI installed by the administrator on the host-side, and the MPI installed inside the container. Both libraries work in tandem to instantiate the job. Hence, the MPI used to compile the application in the container must be compatible with the version of MPI available on the host. In the case of the ",(0,a.kt)("inlineCode",{parentName:"p"},"Bind")," model, we mount the host MPI into the container, without having any MPI library inside the container. The two models, along with their advantages and drawbacks are described in the ",(0,a.kt)("a",{parentName:"p",href:"https://apptainer.org/docs/user/1.0/mpi.html"},"Apptainer User Guide"),"."),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"The present document is inteended to offer some examples and tips for MesoNET supercomputers and should not be taken as an comprehensive User Guide. We encourage you to read and uderstand the ",(0,a.kt)("a",{parentName:"p",href:"https://apptainer.org/docs/user/1.0/mpi.html"},"Apptainer Documentation")," before proceeding.")),(0,a.kt)("h2",{id:"hybrid-model"},"Hybrid Model"),(0,a.kt)("p",null,"The Apptainer documentation provides several useful examples for the ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," model, which can be directly used on the MesoNET supercomputers (i.e. without any additional configurations). Hence "),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"On Slurm supercomputers, it is possible to avoid having two MPI installation instances. Indeed, by using ",(0,a.kt)("inlineCode",{parentName:"p"},"srun")," it is possible to launch containerized MPI applications without having any host-side MPI library. In this case, you totally rely on the MPI library that is inside the container. See examples in ",(0,a.kt)("a",{parentName:"p",href:"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks"},"NVIDIA HPC-Benchmarks"),".")),(0,a.kt)("h2",{id:"bind-model"},"Bind Model"),(0,a.kt)("p",null,"By default, Apptainer automatically mounts several bind points: ",(0,a.kt)("inlineCode",{parentName:"p"},"$HOME"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"/sys"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"/proc"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"/tmp"),", etc. Meaning that you will have access to these points inside the container. These bindings depends on the system configuration and can be different from on machine to another. For more details please visit the (Apptainer Bind Paths and Mounts Documentation)","[https://apptainer.org/docs/user/main/bind_paths_and_mounts.html]",". "),(0,a.kt)("p",null,"Hence, in order to use the host-side MPI library, we need to identify its location, and mount/bind it into the container. Below we provide an example of an Aptainer (Definition File)","[https://apptainer.org/docs/user/main/definition_files.html]"," where we use the host-side MPI implementation to install the well-known (OSU Micro-Benchmarks)","[https://mvapich.cse.ohio-state.edu/benchmarks/]"," inside the container. Then, we run MPI applications inside the container by specifing the installation path to the host-side MPI library via the Apptainer ",(0,a.kt)("inlineCode",{parentName:"p"},"--bind")," option. "),(0,a.kt)("p",null,"On juliet supercomputer, there is a MPI communication library installed at ",(0,a.kt)("inlineCode",{parentName:"p"},"/apps/manual_install/openmpi/4.1.5"),". This location does not correspond to a path that is mounted by default. Hence, we need to manually specify this location when building the container. We create a definition file called ",(0,a.kt)("inlineCode",{parentName:"p"},"mpi_bind.def")," based on an existing public container from Docker Hub. Below is the content of the definition file."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},'\nBootstrap: docker\nFrom: rockylinux:9.2\n\n%setup\n    mkdir -p $APPTAINER_ROOTFS/openmpi/4.1.5\n    mkdir -p $APPTAINER_ROOTFS/bin_host\n    mkdir -p $APPTAINER_ROOTFS/lib64_host\n\n%environment\n    export PATH="$PATH:/bin_host:/openmpi/4.1.5/bin"\n    export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/lib64_host:/openmpi/4.1.5/lib"\n    export OPAL_PREFIX="/openmpi/4.1.5" \n\n%post\n    # install wget\n     dnf install -y wget gcc g++ \n    \n    # relocate MPI installation directory (bind mode)\n    export OPAL_PREFIX="/openmpi/4.1.5" \n    export PATH="$PATH:/bin_host:/openmpi/4.1.5/bin"\n    export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/lib64_host:/openmpi/4.1.5/lib"\n    \n    # get OSU MPI benchmarks\n    mkdir /root/network_benchmarking && cd $_\n    wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.2.tar.gz\n    tar -xf osu-micro-benchmarks-7.2.tar.gz && rm osu-micro-benchmarks-7.2.tar.gz\n    \n    # install OSU MPI benchmarks\n    cd osu-micro-benchmarks-7.2\n    ./configure CC=/openmpi/4.1.5/bin/mpicc CXX=/openmpi/4.1.5/bin/mpicxx \n    make\n')),(0,a.kt)("p",null,"Build the container:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},"export JULIET_MPI_BINDING=/apps/manual_install/openmpi/4.1.5:/openmpi/4.1.5,/usr/bin:/bin_host,/usr/lib64:/lib64_host\napptainer build --bind $JULIET_MPI_BINDING mpi_bind.sif mpi_bind.def\n")),(0,a.kt)("p",null,"Run the container on juliet:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},"export JULIET_MPI_BINDING=/apps/manual_install/openmpi/4.1.5:/openmpi/4.1.5,/usr/bin:/bin_host,/usr/lib64:/lib64_host\nsrun -p mesonet -N 2 -n 2 --mpi=pmi2 apptainer exec --bind $JULIET_MPI_BINDING mpi_bind.sif /root/network_benchmarking/osu-micro-benchmarks-7.2/c/mpi/pt2pt/standard/osu_latency\n")),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"The juliet MPI library depends on the hwloc library, which is located in ",(0,a.kt)("inlineCode",{parentName:"p"},"/usr")," directory. Hence we also need to bind the ",(0,a.kt)("inlineCode",{parentName:"p"},"/usr/bin")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"/usr/lib")," directories.")))}m.isMDXComponent=!0}}]);