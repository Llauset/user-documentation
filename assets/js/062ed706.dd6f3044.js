"use strict";(self.webpackChunkmeso_net=self.webpackChunkmeso_net||[]).push([[704],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>h});var i=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function p(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=i.createContext({}),s=function(e){var n=i.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d=function(e){var n=s(e.components);return i.createElement(l.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},u=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,d=p(e,["components","mdxType","originalType","parentName"]),c=s(t),u=a,h=c["".concat(l,".").concat(u)]||c[u]||m[u]||o;return t?i.createElement(h,r(r({ref:n},d),{},{components:t})):i.createElement(h,r({ref:n},d))}));function h(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,r=new Array(o);r[0]=u;var p={};for(var l in n)hasOwnProperty.call(n,l)&&(p[l]=n[l]);p.originalType=e,p[c]="string"==typeof e?e:a,r[1]=p;for(var s=2;s<o;s++)r[s]=t[s];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}u.displayName="MDXCreateElement"},6065:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>p,toc:()=>s});var i=t(7462),a=(t(7294),t(3905));const o={},r="Apptainer and MPI implementations",p={unversionedId:"juliet/MPI/MPI_and_Apptainer",id:"juliet/MPI/MPI_and_Apptainer",title:"Apptainer and MPI implementations",description:"There are two main ways of launching MPI applications with apptainer, known as Hybrid and Bind models. The Hybrid model involves the use of two MPI libraries: the MPI installed by the administrator on the host-side, and the MPI installed inside the container. Both libraries work in tandem to instantiate the job. Hence, the MPI used to compile the application in the container must be compatible with the version of MPI available on the host. In the case of the Bind model, we mount the host MPI into the container, without having any MPI library inside the container. The two models, along with their advantages and drawbacks are described in the Apptainer User Guide.",source:"@site/docs/juliet/MPI/MPI_and_Apptainer.md",sourceDirName:"juliet/MPI",slug:"/juliet/MPI/MPI_and_Apptainer",permalink:"/documentation/user-documentation/juliet/MPI/MPI_and_Apptainer",draft:!1,editUrl:"https://github.com/MesoNET/user-documentation/tree/main/docs/juliet/MPI/MPI_and_Apptainer.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Building a CUDA-aware Open MPI  library on an Infiniband cluster",permalink:"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide"},next:{title:"NVIDIA HPC-Benchmarks",permalink:"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks"}},l={},s=[{value:"Hybrid Model",id:"hybrid-model",level:2},{value:"Bind Model",id:"bind-model",level:2}],d={toc:s},c="wrapper";function m(e){let{components:n,...t}=e;return(0,a.kt)(c,(0,i.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"apptainer-and-mpi-implementations"},"Apptainer and MPI implementations"),(0,a.kt)("p",null,"There are two main ways of launching MPI applications with apptainer, known as ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"Bind")," models. The ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," model involves the use of two MPI libraries: the MPI installed by the administrator on the host-side, and the MPI installed inside the container. Both libraries work in tandem to instantiate the job. Hence, the MPI used to compile the application in the container must be compatible with the version of MPI available on the host. In the case of the ",(0,a.kt)("inlineCode",{parentName:"p"},"Bind")," model, we mount the host MPI into the container, without having any MPI library inside the container. The two models, along with their advantages and drawbacks are described in the ",(0,a.kt)("a",{parentName:"p",href:"https://apptainer.org/docs/user/1.0/mpi.html"},"Apptainer User Guide"),"."),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"The present document is inteended to offer some examples and tips for MesoNET supercomputers and should not be taken as an comprehensive User Guide. We encourage you to read and uderstand the ",(0,a.kt)("a",{parentName:"p",href:"https://apptainer.org/docs/user/1.0/mpi.html"},"Apptainer Documentation")," before proceeding.")),(0,a.kt)("h2",{id:"hybrid-model"},"Hybrid Model"),(0,a.kt)("p",null,"The Apptainer documentation provides several useful examples for the ",(0,a.kt)("inlineCode",{parentName:"p"},"Hybrid")," model, which can be directly used on the MesoNET supercomputers (i.e. without any additional configurations). Hence "),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"On Slurm supercomputers, it is possible to avoid having two MPI installation instances. Indeed, by using ",(0,a.kt)("inlineCode",{parentName:"p"},"srun")," it is possible to launch containerized MPI applications without having any host-side MPI library. In this case, you totally rely on the MPI library that is inside the container. See examples in ",(0,a.kt)("a",{parentName:"p",href:"/documentation/user-documentation/juliet/benchmarks/Nvidia_HPC_Benchmarks"},"NVIDIA HPC-Benchmarks"),".")),(0,a.kt)("h2",{id:"bind-model"},"Bind Model"),(0,a.kt)("p",null,"By default, Apptainer automatically mounts several bind points: ",(0,a.kt)("inlineCode",{parentName:"p"},"$HOME"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"/sys"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"/proc"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"/tmp"),", etc. Meaning that you will have access to these points inside the container. These bindings depends on the system configuration and can be different from on machine to another. For more details please visit the (Apptainer Bind Paths and Mounts Documentation)","[https://apptainer.org/docs/user/main/bind_paths_and_mounts.html]",". "),(0,a.kt)("p",null,"Hence, in order to use the host-side MPI library, we need to identify its location, and mount/bind it into the container. Below we provide an example of an Aptainer (Definition File)","[https://apptainer.org/docs/user/main/definition_files.html]"," where we use the host-side MPI implementation to install the well-known (OSU Micro-Benchmarks)","[https://mvapich.cse.ohio-state.edu/benchmarks/]"," inside the container. Then, we run MPI applications inside the container by specifing the installation path to the host-side MPI library via the Apptainer ",(0,a.kt)("inlineCode",{parentName:"p"},"--bind")," option. "),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"This example is based on a NVC container. Before proceeding, please make sure that you (configured the NGC Catalog <API_key>)","[../Apptainer/Building_NGC_Containers.md]",".")),(0,a.kt)("p",null,"On juliet supercomputer, there is a CUDA-Aware MPI library installed in ",(0,a.kt)("inlineCode",{parentName:"p"},"/apps/manual_install/openmpi/4.1.5"),". This location does not correspond to a path that is mounted by default. Hence, we need to manually specify this location when building the container. Also, since we have a CUDA-Aware communication library, we need CUDA to be installed inside the container. For this, we create a definition file called ",(0,a.kt)("inlineCode",{parentName:"p"},"apptainer_mpi_cuda.sif")," based on an existing container called ",(0,a.kt)("inlineCode",{parentName:"p"},"nvcr.io/nvidia/cuda:12.2.0-devel-rockylinux9")," from the NGC Catalog, which already\nhas CUDA installed. Below is the content of the definition file."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},'\nBootstrap: docker\nFrom: nvcr.io/nvidia/cuda:12.2.0-devel-rockylinux9\n\n%setup\n        mkdir -p $APPTAINER_ROOTFS/openmpi/4.1.5\n        mkdir -p $APPTAINER_ROOTFS/bin_host\n        mkdir -p $APPTAINER_ROOTFS/lib64_host\n\n%environment\n        export PATH="$PATH:/usr/bin_host:/apps/manual_install/openmpi/4.1.5/bin"\n        export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/lib64_host:/apps/manual_install/openmpi/4.1.5/lib"\n\n%post\n\n    dnf install -y wget gcc g++\n\n    # relocate MPI installation directory (bind mode)\n    export OPAL_PREFIX="/openmpi/4.1.5"\n    export PATH="$PATH:/bin_host:/openmpi/4.1.5/bin"\n    export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/lib64_host:/openmpi/4.1.5/lib"\n\n    # install OSU MPI benchmarks\n    mkdir /root/network_benchmarking && cd $_\n    wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.2.tar.gz\n    tar -xf osu-micro-benchmarks-7.2.tar.gz && rm osu-micro-benchmarks-7.2.tar.gz\n    cd osu-micro-benchmarks-7.2\n    ./configure CC=/openmpi/4.1.5/bin/mpicc CXX=/openmpi/4.1.5/bin/mpicxx --enable-cuda --with-cuda-include=/usr/local/cuda-12.2/include --with-cuda-libpath=/usr/local/cuda-12.2/lib64\n    make\n\n')),(0,a.kt)("p",null,"Build the container on juliet:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},"$> export JULIET_MPI_BINDING=/apps/manual_install/openmpi/4.1.5:/openmpi/4.1.5,/usr/bin:/bin_host,/usr/lib64:/lib64_host\n$> apptainer build --nv --bind $JULIET_MPI_BINDING apptainer_mpi_cuda.sif apptainer_mpi_cuda.def\n")),(0,a.kt)("p",null,"Run the container on juliet:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},"Run the container on juliet:\nexport JULIET_MPI_BINDING=/apps/manual_install/openmpi/4.1.5:/openmpi/4.1.5,/usr/bin:/bin_host,/usr/lib64:/lib64_host\n$> apptainer shell --nv --bind $JULIET_MPI_BINDING apptainer_mpi_cuda.sif\n")),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"The juliet MPI library depends on the hwloc library, which is located in ",(0,a.kt)("inlineCode",{parentName:"p"},"/usr")," directory. Hence we also need to bind the ",(0,a.kt)("inlineCode",{parentName:"p"},"/usr/bin")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"/usr/lib")," directories.")))}m.isMDXComponent=!0}}]);