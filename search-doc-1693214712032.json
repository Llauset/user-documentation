[{"title":"Building a CUDA-aware Open MPI library on an Infiniband cluster","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide","content":"","keywords":"","version":"Next"},{"title":"Setting up and building UCX with GPU supportâ€‹","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-ucx-with-gpu-support","content":"","version":"Next","tagName":"h2"},{"title":"GPU communication performanceâ€‹","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#gpu-communication-performance","content":"To obtain high communication performance on a Infiniband cluster, we first need to enable the GPUDirect RDMA technolog, before building UCX. Prior to CUDA 11.4, GPUDirect RDMA was enabled by installing thenv_peer_memory kernel developped by Mellanox. Starting with CUDA 11.4 there is a new kernel module called nvidia-peermem implemented by Nvidia. To note th\\t nv_peer_memory became deprecated and should be replaced by nvidia-peermem. Please visit the Nvidia website for more details. Additionally, to optimize the intra-node GPU communication latency, UCX should be build with the gdrcopy support. The last one is a library based on the GPUDirect RDMA features. A data transfer performed with gdrcopy is driven by the CPU, and is meant to reduce the communication latency. This is library is composed of a kernel module called gdrdrv and a API called gdrapi. Hence, before building UCX please make sure that both nvidia-peermem and gdrdrv kernel modules are installed and loaded. $ lsmod | grep gdrdrv $ lsmod | grep nvidia_peermem  If the kernel modules are not loaded please refer to the following ressources for installing gdrcopy and/or for loading nvidia-peermem. ","version":"Next","tagName":"h3"},{"title":"Building UCXâ€‹","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-ucx","content":"Building UCX is typically a combination of running &quot;configure&quot; and &quot;make&quot;. For a GPU support we need to specify the cuda and gdrcopy install directories via the --with-cuda and --with-gdrcopy options of &quot;configure&quot;. Below are the steps required to build and install UCX on juliet supercomputer. The latest release UCX tarball can be downloaded from the UCX repository. $ ./configure --prefix=&lt;prefix_path&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-gdrcopy=/apps/manual_install/gdrcopy $ make -j8 install  Once the installtion completed, the information about the current UCX installation instance can be retrived via the ucx_info command. For example, it is possible to check the UCX GPU support via the following command: $ ucx_info -d | grep cuda  To obtain more information it is also possible to change the UCX log level: $ env UCX_LOG_LEVEL=debug ucx_info -d | grep -i cuda  ","version":"Next","tagName":"h3"},{"title":"Setting up and building OpenMPIâ€‹","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-openmpi","content":"Below are the commands for building a CUDA-Aware OpenMPI library with the Slurm support on juliet supercomputer. We need to specify the path to the UCX instalation directory via the --with-ucx option and also the path to cuda via --with-cuda. Additionaly, we need to set the --with-pmi option for supporting slurm (i.e. running MPI application with srun). We also disable the btl openib via --without-verbs option. $ ./configure --prefix=&lt;prefix_path&gt; --with-ucx=&lt;path_to_ucx_install&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-pmi --without-verbs make -j8 install # Check that OpenMPI has been built with CUDA-aware support: $ ompi_info --parsable --all | grep mpi_built_with_cuda_support:value  note Recent OpenMPI versions contain a BTL component called uct, which might cause data corruption when building MPI with UCX. If needed, you can disable uct via the --enable-mca-no-build=btl-uct configuration option. More information about building MPI with UCX can be found here. ","version":"Next","tagName":"h2"},{"title":"OpenMPI performance testsâ€‹","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#openmpi-performance-tests","content":"For testing the performance of a CUDA-Aware OpenMPI installation instance, one can use the well-known OSU benchmarks. ","version":"Next","tagName":"h2"},{"title":"Building OSU Micro-Benchmarksâ€‹","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-osu-micro-benchmarks","content":"$ wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.2.tar.gz $ tar xfp osu-micro-benchmarks-7.2.tar.gz $ cd osu-micro-benchmarks-7.2 $ ./configure CC=mpicc CXX=mpicxx --prefix=&lt;prefix&gt; --enable-cuda $ make install  ","version":"Next","tagName":"h3"},{"title":"Running OSU Micro-Benchmarksâ€‹","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#running-osu-micro-benchmarks","content":"By default, UCX tries to use all available devices on the machine, and selects best ones based on performance characteristics. One can also use manual tunning in order to force the use of certain devices or technologies. For example, we can enable or disable the use of GPUDirect RDMA optimization (available through the nvidia-peermem kernel) and enable or disable the use of gdrcopy (i.e. use of the gdrdrv kernel). Below are a few examples. GPUDirect RDMA and gdrcopyGPUDirect RDMA onlygdrcopy onlyno GDR optimization Both GPUDirect RDMA and gdrcopy enabled (Device to Device) $ mpirun -H juliet3,juliet4 -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/manual_install/gdrcopy/lib -x UCX_IB_GPU_DIRECT_RDMA=1 -x UCX_TLS=all -x UCX_NET_DEVICES=mlx5_0:1 -np 2 c/mpi/pt2pt/standard/osu_latency -d cuda D D # OSU MPI-CUDA Latency Test v7.2 # Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D) # Size Latency (us) # Datatype: MPI_CHAR. 1 4.06 2 4.26 4 4.33 8 4.22 16 4.23 32 4.37 64 4.45 128 4.59 256 4.54 512 4.88 1024 4.89 2048 4.93 4096 5.57 8192 6.20 16384 9.02 32768 12.15 65536 18.42 131072 31.48 262144 56.07 524288 43.67 1048576 74.08 2097152 145.56 4194304 281.57 ```  Manual tunning parametersâ€‹ For all the runs above we need to specify the location of the gdrcopy library via -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/manual_install/gdrcopy/lib. Without this option, the MPI library would be unabled to use gdrcopy, resulting in a performance degradation. We also select the same Infiniband MCA for all our runs via x UCX_NET_DEVICES=mlx5_0:1 . This was done for reproductibility issues. To learn more about possible UCX options and manual tunning please visit the OpenUCX website. ","version":"Next","tagName":"h3"},{"title":"Request a mesonet account","type":0,"sectionRef":"#","url":"/documentation/user-documentation/connectToMesonet","content":"Request a mesonet account You need to create a MesoNET account to access all MesoNET services. Account manager is available at https://iam.mesonet.fr Use first the EduGAIN button, select your institution and your institutional identifiers to connectWithout an institution present in EduGAIN, you can create a local account at https://iam.mesonet.fr/start-registration In both cases, you need to choose a username (you future login) and fill your motivation to join. You will next receive a validation mail : you need to clic on the validation link. Manual validation by a MesoNET administrator will then be required, which may take a few days. ðŸ’¥NOTE : During this process, you may be confronted with an error message. In this case, please stop and restart your browser. tip If you have any questions, please use the support tool and submit a ticket. Access to mesonet services MesoNET website, including description of the ressources : https://www.mesonet.fr GramC portal scientific attribution of the ressources : https://acces.mesonet.fr Temporary portal to access to ressources : https://www.mesonet.fr/portal Documentation portal : TBR Support portal : TBR","keywords":"","version":"Next"}]