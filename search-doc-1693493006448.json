[{"title":"Documentation Utilisateur pour Slurm","type":0,"sectionRef":"#","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm","content":"","keywords":"","version":"Next"},{"title":"1. Introduction √† Slurm‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#1-introduction-√†-slurm","content":"Slurm (Simple Linux Utility for Resource Management) est un puissant syst√®me de gestion de ressources con√ßu pour planifier, surveiller et ex√©cuter des travaux sur des grappes de calcul. Voici quelques concepts de base : Travail (Job) : Une unit√© de travail soumise √† Slurm pour ex√©cution.Partition : Un groupe de n≈ìuds de calcul avec des propri√©t√©s similaires.N≈ìud : Une machine individuelle au sein de la grappe.T√¢che (Task) : Une unit√© d'ex√©cution d'un travail, qui peut √™tre un processus ou un thread.Sbatch : Utilitaire pour soumettre des travaux non interactifs.Srun : Utilitaire pour ex√©cuter des t√¢ches interactives ou non interactives. ","version":"Next","tagName":"h2"},{"title":"2. Soumission de travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#2-soumission-de-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 1 : Soumettre un script simple‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-1--soumettre-un-script-simple","content":"sbatch mon_script.sh  Cette commande soumet le script mon_script.sh pour ex√©cution. Assurez-vous que mon_script.sh contient les directives Slurm appropri√©es en haut du script pour sp√©cifier les ressources n√©cessaires. ","version":"Next","tagName":"h3"},{"title":"Exemple 2 : Soumettre avec allocation de ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-2--soumettre-avec-allocation-de-ressources","content":"sbatch --partition=mesonet --nodes=2 --ntasks-per-node=4 mon_script.sh  Cette commande alloue le travail √† la partition &quot;mesonet&quot; sur 2 n≈ìuds, avec 4 t√¢ches par n≈ìud. Assurez-vous d'ajuster les valeurs selon vos besoins. ","version":"Next","tagName":"h3"},{"title":"3. V√©rification de l'√©tat des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#3-v√©rification-de-l√©tat-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 3 : V√©rifier l'√©tat de vos travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-3--v√©rifier-l√©tat-de-vos-travaux","content":"squeue -u votre_nom_utilisateur  Cette commande affiche la liste des travaux en cours pour l'utilisateur sp√©cifi√©. Vous pouvez voir des informations telles que l'ID du travail, l'√©tat, le n≈ìud, le temps, etc. ","version":"Next","tagName":"h3"},{"title":"Exemple 4 : V√©rifier les travaux de tous les utilisateurs‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-4--v√©rifier-les-travaux-de-tous-les-utilisateurs","content":"squeue  Cette commande affiche la liste de tous les travaux en cours. Utilisez des options suppl√©mentaires pour filtrer les r√©sultats, par exemple, squeue --partition=mesonet pour afficher les travaux dans la partition &quot;mesonet&quot;. ","version":"Next","tagName":"h3"},{"title":"4. Gestion des travaux‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#4-gestion-des-travaux","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 5 : Annuler un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-5--annuler-un-travail","content":"scancel ID_du_travail  Cette commande annule le travail avec l'ID sp√©cifi√©. Vous pouvez obtenir l'ID du travail √† partir de la commande squeue. ","version":"Next","tagName":"h3"},{"title":"Exemple 6 : Modifier la priorit√© d'un travail‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-6--modifier-la-priorit√©-dun-travail","content":"scontrol update JobID ID_du_travail Priority=50  Cette commande modifie la priorit√© du travail avec l'ID sp√©cifi√©. La priorit√© affecte l'ordre d'ex√©cution des travaux. ","version":"Next","tagName":"h3"},{"title":"5. Gestion des ressources‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#5-gestion-des-ressources","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 7 : Sp√©cifier les ressources avec sbatch‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-7--sp√©cifier-les-ressources-avec-sbatch","content":"sbatch --partition=compute --nodes=1 --cpus-per-task=8 --mem=16G mon_script.sh  Cette commande sp√©cifie les ressources pour le travail, y compris la partition, le nombre de n≈ìuds, le nombre de t√¢ches par n≈ìud et le nombre de CPU par t√¢che. ","version":"Next","tagName":"h3"},{"title":"Exemple 8 : Ex√©cuter des t√¢ches interactives avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-8--ex√©cuter-des-t√¢ches-interactives-avec-srun","content":"srun --pty -c 4 /bin/bash  Cette commande lance un shell interactif avec 4 CPU allou√©s. Utile pour les t√¢ches interactives ou les tests. ","version":"Next","tagName":"h3"},{"title":"6. Param√®tres avanc√©s‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#6-param√®tres-avanc√©s","content":"","version":"Next","tagName":"h2"},{"title":"Exemple 9 : Utiliser une r√©servation de n≈ìuds‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-9--utiliser-une-r√©servation-de-n≈ìuds","content":"srun --reservation=ma_reservation --nodes=2 mon_script.sh  Cette commande ex√©cute le travail sur une r√©servation de 2 n≈ìuds sp√©cifi√©e avec l'option --reservation. ","version":"Next","tagName":"h3"},{"title":"Exemple 10 : Utiliser une partition sp√©cifique avec srun‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#exemple-10--utiliser-une-partition-sp√©cifique-avec-srun","content":"srun --partition=visu --nodes=1 mon_script.sh  Cette commande ex√©cute le travail sur la partition &quot;visu&quot; avec 1 n≈ìud. ","version":"Next","tagName":"h3"},{"title":"7. Informations sur les noeuds de calcul‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#7-informations-sur-les-noeuds-de-calcul","content":"sinfo  Cette commande affiche des informations d√©taill√©es sur les n≈ìuds disponibles, telles que leur nom, partition, √©tat, nombre de t√¢ches, m√©moire, ressources, charge CPU, temps d'activit√© et utilisation GPU ","version":"Next","tagName":"h2"},{"title":"8. Ressources suppl√©mentaires‚Äã","type":1,"pageTitle":"Documentation Utilisateur pour Slurm","url":"/documentation/user-documentation/Doc_Utilisateur_Slurm#8-ressources-suppl√©mentaires","content":"https://slurm.schedmd.com/documentation.html ","version":"Next","tagName":"h2"},{"title":"Request a mesonet account","type":0,"sectionRef":"#","url":"/documentation/user-documentation/connectToMesonet","content":"Request a mesonet account You need to create a MesoNET account to access all MesoNET services. Account manager is available at https://iam.mesonet.fr Use first the EduGAIN button, select your institution and your institutional identifiers to connectWithout an institution present in EduGAIN, you can create a local account at https://iam.mesonet.fr/start-registration In both cases, you need to choose a username (you future login) and fill your motivation to join. You will next receive a validation mail : you need to clic on the validation link. Manual validation by a MesoNET administrator will then be required, which may take a few days. üí•NOTE : During this process, you may be confronted with an error message. In this case, please stop and restart your browser. tip If you have any questions, please use the support tool and submit a ticket. Access to mesonet services MesoNET website, including description of the ressources : https://www.mesonet.fr GramC portal scientific attribution of the ressources : https://acces.mesonet.fr Temporary portal to access to ressources : https://www.mesonet.fr/portal Documentation portal : TBR Support portal : TBR","keywords":"","version":"Next"},{"title":"adresses_vm","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/adresses_vm","content":"adresses_vm hostname\tMAC address\tIP addressjuliet5-vm0\t4A:55:4C:49:05:00\t10.12.4.80 juliet5-vm1\t4A:55:4C:49:05:01\t10.12.4.81 juliet5-vm2\t4A:55:4C:49:05:02\t10.12.4.82 juliet5-vm3\t4A:55:4C:49:05:03\t10.12.4.83 juliet5-vm4\t4A:55:4C:49:05:04\t10.12.4.84 juliet5-vm5\t4A:55:4C:49:05:05\t10.12.4.85 juliet5-vm6\t4A:55:4C:49:05:06\t10.12.4.86 juliet5-vm7\t4A:55:4C:49:05:07\t10.12.4.87 juliet5-vm8\t4A:55:4C:49:05:08\t10.12.4.88 juliet5-vm9\t4A:55:4C:49:05:09\t10.12.4.89 juliet5-vm10\t4A:55:4C:49:05:10\t10.12.4.90 juliet5-vm11\t4A:55:4C:49:05:11\t10.12.4.91 juliet5-vm12\t4A:55:4C:49:05:12\t10.12.4.92 juliet5-vm13\t4A:55:4C:49:05:13\t10.12.4.93 juliet5-vm14\t4A:55:4C:49:05:14\t10.12.4.94 juliet5-vm15\t4A:55:4C:49:05:15\t10.12.4.95 juliet5-vm16\t4A:55:4C:49:05:16\t10.12.4.96 juliet5-vm17\t4A:55:4C:49:05:17\t10.12.4.97 juliet5-vm18\t4A:55:4C:49:05:18\t10.12.4.98 juliet5-vm19\t4A:55:4C:49:05:19\t10.12.4.99 juliet5-vm20\t4A:55:4C:49:05:20\t10.12.4.100 juliet5-vm21\t4A:55:4C:49:05:21\t10.12.4.101 juliet5-vm22\t4A:55:4C:49:05:22\t10.12.4.102 juliet5-vm23\t4A:55:4C:49:05:23\t10.12.4.103 juliet5-vm24\t4A:55:4C:49:05:24\t10.12.4.104 juliet5-vm25\t4A:55:4C:49:05:25\t10.12.4.105 juliet5-vm26\t4A:55:4C:49:05:26\t10.12.4.106 juliet5-vm27\t4A:55:4C:49:05:27\t10.12.4.107 juliet5-vm28\t4A:55:4C:49:05:28\t10.12.4.108 juliet5-vm29\t4A:55:4C:49:05:29\t10.12.4.109 juliet5-vm30\t4A:55:4C:49:05:30\t10.12.4.110 juliet5-vm31\t4A:55:4C:49:05:31\t10.12.4.111 juliet6-vm0\t4A:55:4C:49:06:00\t10.12.4.112 juliet6-vm1\t4A:55:4C:49:06:01\t10.12.4.113 juliet6-vm2\t4A:55:4C:49:06:02\t10.12.4.114 juliet6-vm3\t4A:55:4C:49:06:03\t10.12.4.115 juliet6-vm4\t4A:55:4C:49:06:04\t10.12.4.116 juliet6-vm5\t4A:55:4C:49:06:05\t10.12.4.117 juliet6-vm6\t4A:55:4C:49:06:06\t10.12.4.118 juliet6-vm7\t4A:55:4C:49:06:07\t10.12.4.119 juliet6-vm8\t4A:55:4C:49:06:08\t10.12.4.120 juliet6-vm9\t4A:55:4C:49:06:09\t10.12.4.121 juliet6-vm10\t4A:55:4C:49:06:10\t10.12.4.122 juliet6-vm11\t4A:55:4C:49:06:11\t10.12.4.123 juliet6-vm12\t4A:55:4C:49:06:12\t10.12.4.124 juliet6-vm13\t4A:55:4C:49:06:13\t10.12.4.125 juliet6-vm14\t4A:55:4C:49:06:14\t10.12.4.126 juliet6-vm15\t4A:55:4C:49:06:15\t10.12.4.127 juliet6-vm16\t4A:55:4C:49:06:16\t10.12.4.128 juliet6-vm17\t4A:55:4C:49:06:17\t10.12.4.129 juliet6-vm18\t4A:55:4C:49:06:18\t10.12.4.130 juliet6-vm19\t4A:55:4C:49:06:19\t10.12.4.131 juliet6-vm20\t4A:55:4C:49:06:20\t10.12.4.132 juliet6-vm21\t4A:55:4C:49:06:21\t10.12.4.133 juliet6-vm22\t4A:55:4C:49:06:22\t10.12.4.134 juliet6-vm23\t4A:55:4C:49:06:23\t10.12.4.135 juliet6-vm24\t4A:55:4C:49:06:24\t10.12.4.136 juliet6-vm25\t4A:55:4C:49:06:25\t10.12.4.137 juliet6-vm26\t4A:55:4C:49:06:26\t10.12.4.138 juliet6-vm27\t4A:55:4C:49:06:27\t10.12.4.139 juliet6-vm28\t4A:55:4C:49:06:28\t10.12.4.140 juliet6-vm29\t4A:55:4C:49:06:29\t10.12.4.141 juliet6-vm30\t4A:55:4C:49:06:30\t10.12.4.142 juliet6-vm31\t4A:55:4C:49:06:31\t10.12.4.143 juliet7-vm0\t4A:55:4C:49:07:00\t10.12.4.144 juliet7-vm1\t4A:55:4C:49:07:01\t10.12.4.145 juliet7-vm2\t4A:55:4C:49:07:02\t10.12.4.146 juliet7-vm3\t4A:55:4C:49:07:03\t10.12.4.147 juliet7-vm4\t4A:55:4C:49:07:04\t10.12.4.148 juliet7-vm5\t4A:55:4C:49:07:05\t10.12.4.149 juliet7-vm6\t4A:55:4C:49:07:06\t10.12.4.150 juliet7-vm7\t4A:55:4C:49:07:07\t10.12.4.151 juliet7-vm8\t4A:55:4C:49:07:08\t10.12.4.152 juliet7-vm9\t4A:55:4C:49:07:09\t10.12.4.153 juliet7-vm10\t4A:55:4C:49:07:10\t10.12.4.154 juliet7-vm11\t4A:55:4C:49:07:11\t10.12.4.155 juliet7-vm12\t4A:55:4C:49:07:12\t10.12.4.156 juliet7-vm13\t4A:55:4C:49:07:13\t10.12.4.157 juliet7-vm14\t4A:55:4C:49:07:14\t10.12.4.158 juliet7-vm15\t4A:55:4C:49:07:15\t10.12.4.159 juliet7-vm16\t4A:55:4C:49:07:16\t10.12.4.160 juliet7-vm17\t4A:55:4C:49:07:17\t10.12.4.161 juliet7-vm18\t4A:55:4C:49:07:18\t10.12.4.162 juliet7-vm19\t4A:55:4C:49:07:19\t10.12.4.163 juliet7-vm20\t4A:55:4C:49:07:20\t10.12.4.164 juliet7-vm21\t4A:55:4C:49:07:21\t10.12.4.165 juliet7-vm22\t4A:55:4C:49:07:22\t10.12.4.166 juliet7-vm23\t4A:55:4C:49:07:23\t10.12.4.167 juliet7-vm24\t4A:55:4C:49:07:24\t10.12.4.168 juliet7-vm25\t4A:55:4C:49:07:25\t10.12.4.169 juliet7-vm26\t4A:55:4C:49:07:26\t10.12.4.170 juliet7-vm27\t4A:55:4C:49:07:27\t10.12.4.171 juliet7-vm28\t4A:55:4C:49:07:28\t10.12.4.172 juliet7-vm29\t4A:55:4C:49:07:29\t10.12.4.173 juliet7-vm30\t4A:55:4C:49:07:30\t10.12.4.174 juliet7-vm31\t4A:55:4C:49:07:31\t10.12.4.175 juliet7-vm32\t4A:55:4C:49:07:32\t10.12.4.176 juliet7-vm33\t4A:55:4C:49:07:33\t10.12.4.177 juliet7-vm34\t4A:55:4C:49:07:34\t10.12.4.178 juliet7-vm35\t4A:55:4C:49:07:35\t10.12.4.179","keywords":"","version":"Next"},{"title":"Nvidia NGC catalog","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/Apptainer/Building_NGC_Containers","content":"","keywords":"","version":"Next"},{"title":"Pull NGC containers with Apptainer‚Äã","type":1,"pageTitle":"Nvidia NGC catalog","url":"/documentation/user-documentation/juliet/Apptainer/Building_NGC_Containers#pull-ngc-containers-with-apptainer","content":"Pulling containers from NGC requires authentication. Trying to pull an image without an authentification token will result in the following error: $&gt; apptainer pull nvidia_hpc_benchmarks.sif docker://nvcr.io/nvidia/hpc-benchmarks:23.5 FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://nvcr.io/nvidia/hpc-benchmarks:23.5: reading manifest 23.5 in nvcr.io/nvidia/hpc-benchmarks: unauthorized: authentication required  To set up an authentification token follow the steps below: Create an account at https://ngc.nvidia.com or Sign In if you already have an accountOnce logged in, generate an API key at https://ngc.nvidia.com/setupExport the Apptainer environment variables. Execute the commands below after replacing &lt;API_key&gt; with your generated API key. You can add these commands to your .bashrc. export APPTAINER_DOCKER_USERNAME='$oauthtoken' export APPTAINER_DOCKER_PASSWORD=&lt;API_key&gt;  Now you are able to pull any NGC container. For example, you can build the NVIDIA HPC-Benchmarks which includes three benchmarks (HPL-NVIDIA, HPL-AI-NVIDIA and HPCG-NVIDIA) along with some GPU-optimized communication libraries: apptainer pull nvidia_hpc_benchmarks.sif docker://nvcr.io/nvidia/hpc-benchmarks:23.5  ","version":"Next","tagName":"h2"},{"title":"Building a CUDA-aware Open MPI library on an Infiniband cluster","type":0,"sectionRef":"#","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide","content":"","keywords":"","version":"Next"},{"title":"Setting up and building UCX with GPU support‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-ucx-with-gpu-support","content":"","version":"Next","tagName":"h2"},{"title":"GPU communication performance‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#gpu-communication-performance","content":"To obtain high communication performance on a Infiniband cluster, we first need to enable the GPUDirect RDMA technology, before building UCX. Prior to CUDA 11.4, GPUDirect RDMA was enabled by installing thenv_peer_memory kernel developped by Mellanox. Starting with CUDA 11.4 there is a new kernel module called nvidia-peermem implemented by Nvidia. To note th\\t nv_peer_memory became deprecated and should be replaced by nvidia-peermem. Please visit the Nvidia website for more details. Additionally, to optimize the intra-node GPU communication latency, UCX should be build with the gdrcopy support. The last one is a library based on the GPUDirect RDMA features. A data transfer performed with gdrcopy is driven by the CPU, and is meant to reduce the communication latency. This library is composed of a kernel module called gdrdrv and a API called gdrapi. Hence, before building UCX please make sure that both nvidia-peermem and gdrdrv kernel modules are installed and loaded. $ lsmod | grep gdrdrv $ lsmod | grep nvidia_peermem  If the kernel modules are not loaded please refer to the following ressources for installing gdrcopy and/or for loading nvidia-peermem. ","version":"Next","tagName":"h3"},{"title":"Building UCX‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-ucx","content":"Building UCX is typically a combination of running &quot;configure&quot; and &quot;make&quot;. For GPU support we need to specify the cuda and gdrcopy install directories via the --with-cuda and --with-gdrcopy options of &quot;configure&quot;. Below are the steps required to build and install UCX on juliet supercomputer. The latest release UCX tarball can be downloaded from the UCX repository. $ ./configure --prefix=&lt;prefix_path&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-gdrcopy=/apps/manual_install/gdrcopy $ make -j8 install  Once the installation is completed, the information about the current UCX installation instance can be retrieved via the ucx_info command. For example, it is possible to check the UCX GPU support via the following command: $ ucx_info -d | grep cuda  To obtain more information it is also possible to change the UCX log level: $ env UCX_LOG_LEVEL=debug ucx_info -d | grep -i cuda  ","version":"Next","tagName":"h3"},{"title":"Setting up and building OpenMPI‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#setting-up-and-building-openmpi","content":"Below are the commands for building a CUDA-Aware OpenMPI library with Slurm support on juliet supercomputer. We need to specify the path to the UCX installation directory via the --with-ucx option and the path to cuda via --with-cuda. Additionaly, we need to set the --with-pmi option for supporting slurm (i.e. running MPI application with srun). We also disable the btl openib via --without-verbs option. $ ./configure --prefix=&lt;prefix_path&gt; --with-ucx=&lt;path_to_ucx_install&gt; --with-cuda=/apps/spack/spack-softwares/linux-rocky9-zen3/gcc-13.1.0/cuda-12.1.1-hhxtp4y7d55t27jbbxwpjxc4t24tgi3h --with-pmi --without-verbs make -j8 install # Check that OpenMPI has been built with CUDA-aware support: $ ompi_info --parsable --all | grep mpi_built_with_cuda_support:value  note Recent OpenMPI versions contain a BTL component called uct, which might cause data corruption when building MPI with UCX. If needed, you can disable uct via the --enable-mca-no-build=btl-uct configuration option. More information about building MPI with UCX can be found here. ","version":"Next","tagName":"h2"},{"title":"OpenMPI performance tests‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#openmpi-performance-tests","content":"For testing the performance of a CUDA-Aware OpenMPI installation instance, one can use the well-known OSU benchmarks. ","version":"Next","tagName":"h2"},{"title":"Building OSU Micro-Benchmarks‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#building-osu-micro-benchmarks","content":"$ wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.2.tar.gz $ tar xfp osu-micro-benchmarks-7.2.tar.gz $ cd osu-micro-benchmarks-7.2 $ ./configure CC=mpicc CXX=mpicxx --prefix=&lt;prefix&gt; --enable-cuda $ make install  ","version":"Next","tagName":"h3"},{"title":"Running OSU Micro-Benchmarks‚Äã","type":1,"pageTitle":"Building a CUDA-aware Open MPI library on an Infiniband cluster","url":"/documentation/user-documentation/juliet/MPI/MPI_CUDA_aware_Installation_Guide#running-osu-micro-benchmarks","content":"By default, UCX tries to use all available devices on the machine, and selects best ones based on performance characteristics. One can also use manual tunning in order to force the use of certain devices or technologies. For example, we can enable or disable the use of GPUDirect RDMA optimization (available through the nvidia-peermem kernel) and enable or disable the use of gdrcopy (i.e. use of the gdrdrv kernel). Below are a few examples. GPUDirect RDMA and gdrcopyGPUDirect RDMA onlygdrcopy onlyno GDR optimization Both GPUDirect RDMA and gdrcopy enabled (Device to Device) $ mpirun -H juliet3,juliet4 -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/manual_install/gdrcopy/lib -x UCX_IB_GPU_DIRECT_RDMA=1 -x UCX_TLS=all -x UCX_NET_DEVICES=mlx5_0:1 -np 2 c/mpi/pt2pt/standard/osu_latency -d cuda D D # OSU MPI-CUDA Latency Test v7.2 # Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D) # Size Latency (us) # Datatype: MPI_CHAR. 1 4.06 2 4.26 4 4.33 8 4.22 16 4.23 32 4.37 64 4.45 128 4.59 256 4.54 512 4.88 1024 4.89 2048 4.93 4096 5.57 8192 6.20 16384 9.02 32768 12.15 65536 18.42 131072 31.48 262144 56.07 524288 43.67 1048576 74.08 2097152 145.56 4194304 281.57 ```  Manual tunning parameters‚Äã For all the runs above we need to specify the location of the gdrcopy library via -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/manual_install/gdrcopy/lib. Without this option, the MPI library could not use gdrcopy, resulting in performance degradation. We also select the same Infiniband MCA for all our runs via x UCX_NET_DEVICES=mlx5_0:1 . This was done for reproducibility issues. To learn more about possible UCX options and manual tunning please visit the OpenUCX website. ","version":"Next","tagName":"h3"}]